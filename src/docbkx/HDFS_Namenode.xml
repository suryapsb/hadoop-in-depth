<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="HDFS_Namenode" xmlns="http://docbook.org/ns/docbook"
         xmlns:xlink="http://www.w3.org/1999/xlink"
         xmlns:xi="http://www.w3.org/2001/XInclude"
         xmlns:svg="http://www.w3.org/2000/svg"
         xmlns:m="http://www.w3.org/1998/Math/MathML"
         xmlns:html="http://www.w3.org/1999/xhtml"
         xmlns:db="http://docbook.org/ns/docbook">

    <title>HDFS NameNode</title>
    <para>
        Namenode is the coordinator in HDFS.  In this chapter we will look at the functions and internals of NameNode.
    </para>


    <sect1>
        <title> Functions of NameNode </title>

        <sect2>
            <title>NameNode maintains the file system meta data</title>
            <para>
                NameNode contains all meta data about the file system.  For example, the file system hierarchy,  file permissions, etc.   This information is vital for navigating the file system and accessing data.
                <note>
                    NameNode does not store the actual data of the file system.  All file data is stored by Data Nodes.
                </note>
            </para>
        </sect2>


        <sect2>
            <title>NameNode has the location information for files in the cluster</title>
            <para>
                As we know, in HDFS files are spread across the cluster.  So how does one know which machine contains which files?  This information is maintained by NameNode.  This information is called 'Block Location data' - because files are chopped into blocks.
            </para>
            <para>
                This 'block map' is not persisted by NameNode.  It dynamically builds this map from 'block reports' sent by Data Nodes.  Each Data Node reports in with what blocks it has  when it joins the cluster.  Also Data Nodes periodically send updated block reports to NameNode.  So NameNode has the latest accurate block information across the cluster.
            </para>
			<inlinegraphic fileref="hdfs_block_map.jpg" format="JPG" width="100%" />
        </sect2>

        <sect2>
            <title>NameNode is a client's first point of contact</title>
            <para>
                When a client wants to access a file, it contacts NameNode first.  NameNode provides the details of the file and redirects the client to the appropriate Data Nodes.  Then the client contacts the Data Node to access the file.
            </para>
        </sect2>
        <sect2>
            <title>NameNode does not get involved in the data path</title>
            <para>
                The NameNode serves as a reference point for files.  When a client needs to read a file it actually contacts the Data Nodes and streams data directly from Data Nodes.  The NameNode does not get involved in the data handling path.  The reason for this is obvious, we don't want the NameNode to be a bottleneck.
            </para>
        </sect2>
    </sect1>

    <sect1>
        <title>NameNode Architecture</title>
        <sect2>
            <title>What data does the NameNode store?</title>
            <para>
                We said the NameNode contains file system meta data.  What exactly is it?  There are two parts to it.  One, it keeps information about the file system (meta data) such as the file system hierarchy, permissions, etc.   Second, it keeps the location information of where the files are.
            </para>
            <para>
                So where is this information stored?  NameNode stores this data in memory so NameNode can access this information very quickly.  But how does this information survive a NameNode restart?  NameNode persists this information on disk as well.
            </para>
            <para>
                As we saw, NameNode is very critical for the cluster operation, so we cannot risk losing NameNode data.  We are already storing the data to disk.  What if that disk fails?
            </para>
            <para>
                We will store this data on two disks.
            </para>
            <para>
                What if the machine that NameNode is running catches fire and takes down both the disks?  We can write to remote storage.  It could be an external filer that can be mounted by NFS.  So a copy of data is available even if the NameNode machine melts down!
            </para>
            <para>
                So on production systems, NameNode is configured to save the data at multiple locations:
                <itemizedlist>
                    <listitem>
                        a disk on the NameNode host
                    </listitem>
                    <listitem>
                        another disk on the NameNode host
                    </listitem>
                    <listitem>
                        a remote filer (usually mounted by NFS)
                    </listitem>
                </itemizedlist>
                This way, we can be sure the precious NameNode data is saved against disk / host failures.
            </para>
            <para>
                So lets see how NameNode updates its data.  The following diagram shows how file system data is modified.

                <orderedlist>
                    <listitem>
                        a client makes a request (say, to create a file)
                    </listitem>
                    <listitem>
                        NameNode first updates  disk images
                        <itemizedlist>
                            <listitem>
                                Writes to disk 1
                            </listitem>
                            <listitem>
                                Writes to disk 2
                            </listitem>
                            <listitem>
                                Writes to remote disk / filer (via an NFS mount)
                            </listitem>
                        </itemizedlist>
                    </listitem>

                    <listitem>
                        it then updates its in-memory data structures
                    </listitem>

                    <listitem>
                        after all this is done, it returns a SUCCESS code to the client
                    </listitem>
                </orderedlist>
                Why is data on disk persisted before in-memory data structures?  This way, even if the NameNode goes down between writes, the meta data will be in a consistent state.
				<inlinegraphic fileref="nn_1.jpg" format="JPG" width="100%" />
            </para>
        </sect2>
        <sect2>
            <title>How is the meta data stored?</title>
            <para>
                We learned NameNode saves meta data on disk.  On a busy cluster there could be thousands of file system operations (create, delete, move) per minute and, on a big cluster, the size of the meta data could approach gigabytes.  Updating such a large file would be slow and unwieldy.  So how does the NameNode solve this?
            </para>
            <para>
                The approach consists of Snapshots and EditLogs.  The process is like this:

                <orderedlist>
                    <listitem>
                        When NameNode boots up, it loads the latest Snapshot of the meta data.
                    </listitem>
                    <listitem>
                        As it handles file system changes, it writes to a separate file called EditLog.  As EditLog exceeds a certain size, a new EditLog file is created.
                    </listitem>
                    <listitem>
                        Next time NameNode boots, it loads the latest snapshot and replays the EditLogs until it is up to date on file system status.
                    </listitem>
                    <listitem>
                        Once all EditLogs are replayed, then NameNode is ready to accept file system operations.  They will be recorded in EditLogs as well.
                    </listitem>
                    <listitem>
                        This process repeats many times during a NameNode's life cycle.
                    </listitem>
                </orderedlist>
            </para>

            <para>
                As you can imagine, the EditLogs can add up in size and replaying them each time NameNode boots up can take longer and longer.  It makes sense to create new snapshots out of EditLogs.
				<inlinegraphic fileref="nn_editlogs.jpg" format="JPG" width="100%" />
                This is where the Secondary NameNode comes into play.
            </para>
        </sect2>

        <sect2>
            <title>Secondary Namenode</title>
            <para>
                A Secondary NameNode is another daemon that is responsible for merging edit logs into snapshots.   The name 'secondary' can mislead people into thinking this is a 'backup' NameNode, and can take over when NameNode goes down.  That is not what Secondary NameNode does.  It is actually a 'sidekick' for NameNode. 
            </para>
            <para>
                Here is what Secondary NameNode does:
                <orderedlist>
                    <listitem>
                        Contacts NameNode and gets latest Snapshot.  It uses HTTP to communicate.
                    </listitem>
                    <listitem>
                        Also gets EditLogs from NameNode
                    </listitem>
                    <listitem>
                        Loads both snapshots and EditLogs into memory, applies EditLogs to the Snapshot, creates another Snapshot.
                    </listitem>
                    <listitem>
                        Uploads the newly minted Snapshot back to NameNode.
                    </listitem>
                    <listitem>
                        NameNode accepts the new Snapshot and replaces its current one.
                    </listitem>
                    <listitem>
                        This process repeats.
                    </listitem>
                </orderedlist>
				<inlinegraphic fileref="snn.jpg" format="JPG" width="100%" />
            </para>
            <para>
                So why make Secondary NameNode another daemon instead of part of NameNode?
                <sbr/>
                This is for two reasons:
                <itemizedlist>
                    <listitem>
                        To merge the snapshots and EditLogs into a new snapshot, they have to be loaded into memory.   On a large cluster, the image files can be quite large (gigabytes) and loading them into memory can consume large amounts of memory.  By moving it to another host, we can minimize the memory needs of NameNode.
                    </listitem>

                    <listitem>
                        Also on a large, busy cluster the EditLogs can be large and merging them can be very resource consuming.  By offloading to another host, NameNode is not overloaded.
                    </listitem>
                </itemizedlist>
            </para>
            <para>
                On a large cluster (more than 25 nodes) it is advisable to run Secondary Namenode on a separate host.  For smaller clusters, Secondary NameNode can be run on the same node as NameNode
            </para>
        </sect2>

    </sect1>
</chapter>
