<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="Handy_Classes" xmlns="http://docbook.org/ns/docbook">
  <title>Handy Classes</title>
  <note>
      The original article was published by Sujee Maniyam at his website: <ulink  url="http://sujee.net/tech/articles/hadoop/hadoop-useful-classes/">here</ulink>
  </note>
  <para>
      These are some handy 'utility' classes bundled with Hadoop.
  </para>

  <sect1>
      <title>Executing Shell Commands</title>
      <para>
          class : org.apache.hadoop.util.Shell / org.apache.hadoop.util.Shell.ShellCommandExecutor <sbr />
      jar : hadoop1 : HADOOP_HOME/lib/hadoop-core.jar <sbr/>
      jar : hadoop 2 : HADOOP_HOME/share/hadoop/common/hadoop-common.jar <sbr/>
      </para>

      <para>
          Why would we want to execute an outside executable from within a Hadoop client or map reduce?  Perhaps we have a program that converts images into another format (jpg --> png).  Rather than rewriting this in Java, we can use a shell to invoke the command:
      </para>

      <programlisting language="java">
import org.apache.hadoop.util.Shell.ShellCommandExecutor;

String[] cmd =    { "ls", "/usr" };
ShellCommandExecutor shell = new ShellCommandExecutor(cmd);
shell.execute();
System.out.println("* shell exit code : " + shell.getExitCode());
System.out.println("* shell output: \n" + shell.getOutput());

/* output:
* shell exit code : 0
* shell output:
X11
X11R6
bin
etc
include
lib
libexec
llvm-gcc-4.2
local
sbin
share
stand alone
*/

      </programlisting>
  </sect1>

  <sect1>
      <title>StringUtils</title>
      class : org.apache.hadoop.util.StringUtils  <sbr/>
     jar : hadoop 1 :  HADOOP_HOME/lib/hadoop-core.jar <sbr/>
     jar : hadoop 2 :  HADOOP_HOME/share/hadoop/client/lib/hadoop-common.jar <sbr/>

     <sect2>
         <title>
            StringUtils.byteDesc() : User-friendly / human-readable byte lengths
         </title>
         <para>
            How many megabytes is 10000000 bytes?   this will tell you.
        </para>
      <programlisting language="java">
import org.apache.hadoop.util.StringUtils;

// --- human readable  byte lengths -----
System.out.println ("1024 bytes = " + StringUtils.byteDesc(1024));
System.out.println ("67108864 bytes = " + StringUtils.byteDesc(67108864));
System.out.println ("1000000 bytes = " + StringUtils.byteDesc(1000000));

/* produces:
1024 bytes = 1 KB
67108864 bytes = 64 MB
1000000 bytes = 976.56 KB
*/

      </programlisting>
     </sect2>


     <sect2>
         <title>StringUtils.byteToHexString() : Convert Bytes to Hex strings and vice-versa</title>
         <para>
             We deal with byte arrays in Hadoop / map reduce.  This is a handy way to print / debug issues
         </para>

      <programlisting language="java">
          <![CDATA[
import org.apache.hadoop.util.StringUtils;

// ----------------- String Utils : bytes <--> hex ---------
String s = "aj89y1_xxy";
byte[] b = s.getBytes();
String hex = StringUtils.byteToHexString(b);
byte[] b2 = StringUtils.hexStringToByte(hex);
String s2 = new String(b2);
System.out.println(s + " --> " + hex + " <-- " + s2);

/* output:
aj89y1_xxy --> 616a383979315f787879 <-- aj89y1_xxy
*/
]]>
      </programlisting>

     </sect2>


     <sect2>
         <title>StringUtils.formatTime() :  human readable elapsed time</title>
         <para>
How long is 100000000 ms?   See below:
         </para>

      <programlisting language="java">
System.out.println ("1000000 ms = " + StringUtils.formatTime(1000000));
long t1 = System.currentTimeMillis();
// execute a command
long t2 = System.currentTimeMillis();
t2 += 10000000; // adjust for this demo
System.out.println ("operation took : " + StringUtils.formatTimeDiff(t2, t1));

/* output:
1000000 ms = 16mins, 40sec
operation took : 2hrs, 46mins, 40sec
*/

      </programlisting>

     </sect2>
  </sect1>


  <sect1>
      <title>Hadoop Cluster Status</title>
      class : org.apache.hadoop.mapred.ClusterStatus <sbr/>
      jar : hadoop 1 : HADOOP_HOME/lib/hadoop-core.jar <sbr/>
      jar : hadoop 2 : HADOOP_HOME/share/hadoop/client/lib/hadoop-mapreduce-client-core.jar
      <para>
          Find out how many nodes are in the cluster, how many mappers, reducers, etc.
      </para>

      <programlisting language="java">
package hi;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.mapred.ClusterStatus;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobConf;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;

/*
compile into a jar and execute:
     hadoop jar a.jar hi.TestCluster

output:
number of task trackers: 1
maximum map tasks : 3
currently running map tasks : 0

*/

public class TestCluster extends Configured implements Tool
{
    public static void main(String[] args) throws Exception
    {
        int result = ToolRunner.run(new Configuration(), new TestCluster(), args);
        System.exit(result);
    }

    @Override
    public int run(String[] arg0) throws Exception
    {
        JobConf dummyJob = new JobConf(new Configuration(), TestCluster.class);
        JobClient jobClient = new JobClient(dummyJob);
        ClusterStatus clusterStatus = jobClient.getClusterStatus();
        System.out.println("number of task trackers: " + clusterStatus.getTaskTrackers());
        System.out.println("maximum map tasks : " + clusterStatus.getMaxMapTasks());
        System.out.println("currently running map tasks : " + clusterStatus.getMapTasks());
        return 0;
    }
}

      </programlisting>

  </sect1>




</chapter>
