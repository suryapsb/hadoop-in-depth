<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="Hadoop_V1_V2" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">
    <title>How Hadoop version 2 'looks' different from version 1</title>
    <para>
        This chapter details how Hadoop version 2 'looks and feels' different from version 1.  To learn about functional differences between version 1 and 2, look at chapters see <xref linkend="hadoop_versions" />.
    </para>

    <section>
        <title>start/stop  scripts</title>
        <para>
            These are files used to start/stop Hadoop daemons:
            <command>

            start-dfs.sh, stop-dfs.sh, start-mapred.sh, stop-mapred.sh, start-all.sh, stop-all.sh,  hadoop-daemon.sh,  hadoop-daemons.sh
            </command>
        </para>
        <para>
            In Hadoop v1 the start/stop scripts are in the <command>HADOOP_HOME/bin</command> dir.  In Hadoop2 they are in the  <command>HADOOP_HOME/sbin</command> dir
        </para>
        <para>
            Note: in the following tables ""HH" stands for the Hadoop install directory.
        </para>

        <section>
            <title>Tar Package Version</title>
            <table id="hadoop-v1-v2-start-stop-scrips-tar">
                <title>Start / Stop scripts for tar package version</title>
                <tgroup cols='3' align='left' colsep='1' rowsep='1'>
                    <thead>
                        <row>
                            <entry>Task</entry>
                            <entry>Hadoop 1</entry>
                            <entry>Hadoop 2</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                to start HDFS
                            </entry>
                            <entry>
                                <command>$ HH/bin/start-dfs.sh</command>
                                <sbr/>
                                <command>$ HH/bin/hadoop-daemon.sh start namenode</command>
                            </entry>
                            <entry>
                                <command>$ HH/sbin/start-dfs.sh</command>
                                <sbr/>
                                <command>$ HH/sbin/hadoop-daemon.sh start namenode</command>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                to start Map Reduce
                            </entry>
                            <entry>
                                <command>$ HH/bin/start-mapred.sh</command>
                            </entry>
                            <entry>
                                <command>$ HH/sbin/start-yarn.sh</command>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                to start everything
                            </entry>
                            <entry>
                                <command>$ HH/bin/start-all.sh</command>
                            </entry>
                            <entry>
                                <command>$ HH/sbin/start-all.sh</command>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </section>

        <section>
            <title>RPM Version</title>
            <table id="hadoop-v1-v2-start-stop-scrips-rpm" grid="all">
                <title>Start / Stop scripts for rpm package version</title>
                <tgroup cols='3' align='left' colsep='1'>
                    <thead>
                        <row>
                            <entry>Task</entry>
                            <entry>Hadoop 1</entry>
                            <entry>Hadoop 2</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                to start HDFS
                            </entry>
                            <entry>
                                On NameNode:<sbr/>
                                <command>
                                    sudo service hadoop-0.20-namenode start
                                </command>
                                <sbr/>
                                <sbr/>
                                On Data Node:<sbr/>
                                <command>
                                    sudo service hadoop-0.20-datanode start
                                </command>
                            </entry>
                            <entry>
                                On NameNode:<sbr/>
                                <command>
                                    sudo service hadoop-hdfs-namenode  start
                                </command>
                                <sbr/>
                                <sbr/>
                                On Data Node:<sbr/>
                                <command>
                                    sudo service hadoop-hdfs-datanode  start
                                </command>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                to start Map Reduce
                            </entry>
                            <entry>
                                on Job Tracker: <sbr/>
                                <command>
                                    sudo service hadoop-0.20-jobtracker start
                                </command>
                                <sbr/>
                                <sbr/>
                                on Task Tracker: <sbr/>
                                <command>
                                    sudo service hadoop-0.20-tasktracker start
                                </command>
                            </entry>
                            <entry>
                                on Job Tracker: <sbr/>
                                <command>
                                    sudo service hadoop-yarn-resourcemanager restart
                                </command>
                                <sbr />
                                <command>
                                    sudo service hadoop-mapreduce-historyserver start
                                </command>
                                <sbr />
                                <command>
                                    sudo service hadoop-yarn-proxyserver start
                                </command>

                                <sbr/>
                                <sbr/>
                                on "worker nodes": <sbr/>
                                <command>
                                    sudo service hadoop-yarn-nodemanager start
                                </command>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
        </section>


    </section>

    <section>
        <title>Hadoop Command Split</title>
        <para>
            In v1 there are bin/hadoop executables used for file system operations, administration and map reduce operations.  In v2 these are handled by separate binaries.
        </para>
            <table id="hadoop-v1-v2-hadoop-command-split" grid="all">
                <title>Hadoop Command Split</title>
                <tgroup cols='3' align='left' colsep='1' rowsep='1'>
                    <thead>
                        <row>
                            <entry>Task</entry>
                            <entry>Hadoop 1</entry>
                            <entry>Hadoop 2</entry>
                        </row>
                    </thead>
                    <tbody>
                        <row>
                            <entry>
                                File system operations
                            </entry>
                            <entry>
                                <command>$ HH/bin/hadoop dfs -ls</command>
                            </entry>
                            <entry>
                                <command>$ HH/bin/hdfs dfs -ls</command>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                NameNode operations
                            </entry>
                            <entry>
                                <command>$ HH/bin/hadoop namenode -format</command>
                            </entry>
                            <entry>
                                <command>$ HH/bin/hdfs namenode -format</command>
                            </entry>
                        </row>
                        <row>
                            <entry>
                                File system admin commands
                            </entry>
                            <entry>
                                <command>$ HH/bin/hadoop dfsadmin -refreshNodes</command>
                            </entry>
                            <entry>
                                <command>$ HH/bin/hdfs dfsadmin -refreshNodes</command>
                            </entry>
                        </row>
                    </tbody>
                </tgroup>
            </table>
    </section>

    <section>
        <title>Config Files</title>
        <para>
            Sample files: core-site.xml, hdfs-site.xml, mapred-site.xml
        </para>
        <para>
            In Hadoop v1 the config files are in the HADOOP_HOME/<emphasis>conf</emphasis> directory.  In Hadoop v2 they are in the  HADOOP_HOME/<emphasis>etc/hadoop</emphasis> directory.  The files are all intact at their new location.
        </para>
    </section>

    <section>
        <title>Lib Dir Split</title>
        <para>
            In Hadoop v1 all dependency jars are in the lib dir.  In v2 they are broken out into sub directories.
            <sbr/>
            HH/share/hadoop/common
            <sbr/>
            HH/share/hadoop/hdfs
            <sbr/>
            HH/share/hadoop/mapreduce
            <sbr/>
            HH/share/hadoop/tools
            <sbr/>
            HH/share/hadoop/yarn
            <sbr/>

            Each of these directories also has a lib sub folder for their own dependencies.  This means there may be duplicate copies of some jars.  For example
            <sbr/>
            find hadoop-2  -name "log4j*.jar" yields
            <sbr/>
            hadoop-2/share/hadoop/common/lib/log4j-1.2.17.jar
            <sbr/>
            hadoop-2/share/hadoop/hdfs/lib/log4j-1.2.17.jar
            <sbr/>
            hadoop-2/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/log4j-1.2.17.jar
            <sbr/>
            hadoop-2/share/hadoop/mapreduce/lib/log4j-1.2.17.jar
            <sbr/>
            hadoop-2/share/hadoop/yarn/lib/log4j-1.2.17.jar
            <sbr/>
            This is okay, as now each component (hdfs, mapreduce, yarn) can have it dependencies in a clean way.
        </para>
        <para>
            The split of jar files means that we have to change run scripts also.  Here is an example of a java program that connects with HDFS:
            <sbr/>
            <code>
                java -cp my.jar:$hadoop_home/share/hadoop/hdfs/*:$hadoop_home/share/hadoop/hdfs/lib/*:$hadoop_home/share/hadoop/common/*:$hadoop_home/share/hadoop/common/lib/*  MyClass
            </code>
        </para>
    </section>

    <section>
        <title>Running TestDFSIO</title>
        <para>
            in v1
            <sbr/>
            <code>
                $  hadoop jar HH/hadoop-tests.jar TestDFSIO  -write -nrFiles 10  -fileSize 1000
            </code>
        </para>
        <para>
            in v2
            <sbr/>
            <code>
                $ hadoop jar HH/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-*-tests.jar TestDFSIO  -write -nrFiles 4 -fileSize 1GB
            </code>
        </para>
    </section>

</chapter>
