<?xml version="1.0" encoding="UTF-8"?>
<chapter version="5.0" xml:id="MapReduce_Counters" xmlns="http://docbook.org/ns/docbook"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:m="http://www.w3.org/1998/Math/MathML"
    xmlns:html="http://www.w3.org/1999/xhtml"
    xmlns:db="http://docbook.org/ns/docbook">

    <title>MapReduce Counters</title>
    <section>
        <title>About</title>
        <para>
            One of the built in features of Map Reduce is counters. Counters are basically statistics of a map reduce job.  These counters can be very valuable in gaining insight into a job.
        </para>
        <para>
            Here is an example of counters from a map reduce job
        </para>
        <mediaobject>
            <imageobject>
                <imagedata fileref="counters_1.png" format="PNG" scale="100" />
            </imageobject>
        </mediaobject>

        <para>
            For example, the 'File System Counters' show how much HDFS data is read (HDFS_BYTES_READ)  and written (HDFS_BYTES_WRITTEN).
        </para>
        <para>
            Another quick look at 'Map-Reduce Framework' counters such as 'Map Input Records'  and 'Map Output Records'
        </para>

        <para>
            These counters are 'distributed'.  Each node sends up counter values to Job Tracker.  Job Tracker collects and aggregates counter values  and updates.
        </para>
    </section>


    <section>
        <title>Uses of Counters</title>
        <para>
            Counters can give an insight into a job.  And these counters are readily available, without us having to write any extra code.  So it is a good habit to check these counters.  Here are some sample debug scenarios.
        </para>
        <glosslist>
            <glossentry>
                <glossterm>Verifying Input</glossterm> : <sbr/>
                <glossdef>
                    <para>
                        HDFS_BYTES_READ : this counter indicates how much data is read by the job.  If the input data size is 1GB (example) this value should be 1,000,000,000 bytes.  If this counter does not match expectation, then there might be something wrong with our input
                    </para>
                </glossdef>
            </glossentry>

            <glossentry>
                <glossterm>Map output and reducer input</glossterm> <sbr/>
                <glossdef>
                    <para>
                        One of the common mistakes in map-reduce programming is, the map / reduce types mis-match;  Then the output from mappers won't get processed by reducers.
                    </para>
                    <para>
                        To test this  compare two counters : 'Map Output Records'  and 'Reduce Input Records'.  They should be the same.
                    </para>
                </glossdef>
            </glossentry>

            <glossentry>
                <glossterm>Performance check : data locality</glossterm> <sbr/>
                <glossdef>
                    <para>
                        One of the principles of Map Reduce is data locality -- maps operate on data available on the same node -- well mostly.   How can we inspect how many map tasks were local?  'Data-local map tasks' will tell you that.  In this case no data needs to be copied.  The next level is 'Rack-local map tasks' -- maps.  In this case data needs to be copied, but within the rack.
                    </para>
                </glossdef>
            </glossentry>
            <glossentry>
                <glossterm>Performance check : compression</glossterm> <sbr/>
                <glossdef>
                    <para>
                        Lot of temporary data is generated between map phase and reduce phase.  This data has to be streamed across network from mappers to reducers.  To minimize the volume of this data, we can compress it.  How do we verify the compression is in effect?
                    </para>
                    <para>
                        the counter to check is 'Map output bytes'.  Take a reading of this counter  after a first run.  Do a second run with compression turned on and take the reading.  The value of this counter should be significantly less.
                    </para>
                </glossdef>
            </glossentry>
        </glosslist>

    </section>

    <section>
        <title>How do Counters Work?</title>
        <para>
            For example, lets consider the counter HDFS_BYTES_READ.  It reports the total number of bytes read from HDFS.  How is this calculated?
        </para>
        <para>
            Map or Reduce jobs run on Task Trackers (on worker nodes).  When the tasks read data from HDFS they keep a count.  Now these counts are kept in memory of individual task trackers.  The Task Trackers send these counters up to Job Tracker.  The Job Tracker collects counters from Task Trackers and aggregates them.
        </para>
        <para>
            The following diagram shows how task trackers keep their own count and the Job Tracker keeps the 'master' count
        TODO : counter picture
        </para>
    </section>


    <section>
        <title>User Defined Counters</title>
        <para>
            We have seen counters provided already by Map Reduce and File Systems.  Map Reduce also allows to define custom counters in map reduce programs.
        </para>
    </section>

</chapter>
